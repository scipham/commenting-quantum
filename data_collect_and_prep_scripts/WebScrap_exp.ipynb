{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_struct as dm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "#import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"S:\\\\Sync\\\\University\\\\2023_MRP_1\\\\MRP1_WorkDir\\\\data\\\\\"\n",
    "\n",
    "dm.Scraper.query_from_subreddits(query=\"quantum self:no\",\n",
    "                                 subreddits=\n",
    "\n",
    "#Convert to a standardized object structure before exporting to csv files:\n",
    "artset = dm.ArticleSet.from_reddit_dfs(artset_descript=\"Test Reddit ArticleSet\", \n",
    "                                    post_df = post_df, \n",
    "                                    comment_df = comment_df,\n",
    "                                    scraper=scraper,)\n",
    "\n",
    "#print(artset.as_list[0].comments[2])\n",
    "\n",
    "artset.save_to_csv(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df[\"depth\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR store directly in csv files via a direct mapping:\n",
    "reddit_postFeatures = [\"id\",\"ext_link_url\"]\n",
    "reddit_commentFeatures = [\"id\", \"parent_post_id\", \"created_utc\", \"body\", \"depth\",\"parent_comment_id\" , \"replies_ids\"]\n",
    "\n",
    "target_articleFeatures = [\"article_id\", \"article_type\", \"link\", \"date\", \"author\", \"headline\", \"body\", \"char_count\", \"comments_ids\"] # [field.name for field in fields(Article)].remove(\"comments\")\n",
    "target_commentFeatures =  [\"comment_id\", \"article_id\", \"date\", \"username\", \"from_where\" ,\"body\", \"level\" , \"parent_comment_id\", \"replies_ids\"]\n",
    "direct_target_articleFeatures = target_articleFeatures.copy().remove(\"article_type\", \"date\", \"author\", \"headline\", \"body\", \"char_count\", \"comments_ids\") #Subset of target_articleFeatures which can be directly mapped from the reddit data\n",
    "direct_target_commentFeatures = target_commentFeatures.copy() #Subset of target_commentFeatures which can be directly mapped from the reddit data\n",
    "\n",
    "reddit_to_commentdf_map = dict(zip(direct_target_articleFeatures, reddit_postFeatures))\n",
    "reddit_to_articledf_map = dict(zip(direct_target_commentFeatures, reddit_commentFeatures))\n",
    "\n",
    "export_article_df = pd.DataFrame(columns=target_articleFeatures)\n",
    "export_comment_df = pd.DataFrame(columns=target_commentFeatures)\n",
    "export_article_df.loc[:, reddit_to_articledf_map.keys()] = post_df.loc[:, reddit_to_articledf_map.values()]\n",
    "export_comment_df.loc[:, reddit_to_commentdf_map.keys()] = comment_df.loc[:, reddit_to_commentdf_map.values()]\n",
    "\n",
    "export_article_df.loc[:,\"article_type\"] = \"web_article\"\n",
    "#Parse: \"date\", \"author\", \"headline\", \"body\" from ext link url\n",
    "# \n",
    "export_article_df.loc[:,\"char_count\"] = len(export_article_df.loc[:,\"body\"]) \n",
    "# Map \"comments_ids\" to the articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China to launch hack-proof quantum communication network in 2016 ... Stephen Chen Updated ... 2014-11-04\n",
      "China will complete and put into service the world's longest quantum communication network stretching 2,000km from Beijing to Shanghai by 2016, say scientists leading the project.\n",
      "China to launch hack-proof quantum communication network in 2016\n",
      "China will complete and put into service the world's longest quantum communication network stretching 2,000km from Beijing to Shanghai by 2016, say scientists leading the project.\n"
     ]
    }
   ],
   "source": [
    "from trafilatura import fetch_url, extract, bare_extraction\n",
    "from courlan import validate_url, check_url, clean_url\n",
    "import requests\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "#url = 'https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/'\n",
    "url = \"https://scottaaronson.blog/?p=6871\"\n",
    "#url = \"https://tweakers.net/nieuws/189740/qutech-start-quantum-network-explorer-voor-publieke-experimenten-quantumnetwerk.html\"\n",
    "#url = \"https://www.quantamagazine.org/how-quantum-physicists-flipped-time-and-how-they-didnt-20230127/\"\n",
    "#url = \"https://news.st-andrews.ac.uk/archive/st-andrews-scientists-turn-up-the-heat-on-physics-phenomenon/#:~:text=A%20'quantum%20harmonic%20oscillator'%20%E2%80%93,the%20University%20of%20St%20Andrews.\"\n",
    "#url = \"https://www.vice.com/en/article/88qj3z/government-scientists-discover-entirely-new-kind-of-quantum-entanglement-in-breakthrough\"\n",
    "#url = \"https://www.nature.com/articles/s41534-022-00631-2\"\n",
    "#url = \"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiX073B8vH8AhUThP0HHe5FDd8QtwJ6BAgSEAI&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DjHoEjvuPoB8&usg=AOvVaw2oWlY57R1_JHg0bgTgPkoW\"\n",
    "#url = \"https://youtu.be/jHoEjvuPoB8\"\n",
    "#url = \"https://www.scmp.com/news/china/article/1631479/china-launch-hack-proof-quantum-communication-network-2016?page=all\"\n",
    "\n",
    "\n",
    "assert validate_url(url)[0] == True #check whether url can be resolved or is damaged\n",
    "\n",
    "url = clean_url(url)\n",
    "downloaded = fetch_url(url)\n",
    "\n",
    "result = bare_extraction(downloaded, \n",
    "                            url=url, \n",
    "                            include_comments=True,\n",
    "                            #include_formatting=False,\n",
    "                            favor_precision=True, #favor recall\n",
    "                            with_metadata=True,\n",
    "                            #deduplicate=True, \n",
    "                            only_with_metadata=True,\n",
    "                            #url_blacklist=None,\n",
    "                            #as_dict=True,\n",
    "                            )\n",
    "\n",
    "#print(result.keys())\n",
    "print(result[\"title\"], \"...\", result[\"author\"],\"...\",  result[\"date\"])\n",
    "#print(result[\"description\"])\n",
    "print(result[\"text\"])\n",
    "\n",
    "#print(type(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(result[\"comments\"])\n",
    "comments = re.split(r\"Comment\\s#[0-9]+\\s\", string=result[\"comments\"])[1:]\n",
    "print(len(comments))\n",
    "\n",
    "comment_dicts = [{\"date\": datetime.strptime(re.sub(r\"((st)|(nd)|(rd)|(th)),\", \"\", cmt.split(\"\\n\")[0]), \"%B %d %Y at %I:%M %p\").date(), \"body\": \" \\n\".join(cmt.split(\"\\n\")[1:])} for cmt in comments]\n",
    "tl_comment_dicts = [cmt for cmt in comment_dicts if \"#\" not in \"\".join(cmt[\"body\"].split(\"\\n\")[0:1]) ]\n",
    "\n",
    "print(tl_comment_dicts[2][\"body\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url_list = []\n",
    "for page_n in range(0, 28+1):\n",
    "    downloaded = fetch_url(\"https://scottaaronson.blog/?cat=4&paged=%d\" % page_n)\n",
    "    soup = BeautifulSoup(downloaded, 'html.parser')\n",
    "    post_divs = soup.body.find(id=\"page\").find(id=\"content\").find_all(class_=\"post\")\n",
    "    post_ids = [int(div.h3[\"id\"].split(\"-\")[-1]) for div in post_divs]\n",
    "    post_urls = [\"https://scottaaronson.blog/?p=%d\" % post_id for post_id in post_ids]\n",
    "    url_list.extend(post_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"urllist_cache.pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(url_list, fp)\n",
    "\n",
    "cached_url_list = []\n",
    "with open(\"urllist_cache.pkl\", \"rb\") as fp:   # Unpickling\n",
    "    cached_url_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "Done with article 0 of 290\n",
      "141\n",
      "Done with article 1 of 290\n",
      "32\n",
      "Done with article 2 of 290\n",
      "25\n",
      "Done with article 3 of 290\n",
      "6\n",
      "Done with article 4 of 290\n",
      "18\n",
      "Done with article 5 of 290\n",
      "20\n",
      "Done with article 6 of 290\n",
      "16\n",
      "Done with article 7 of 290\n",
      "25\n",
      "Done with article 8 of 290\n",
      "70\n",
      "Done with article 9 of 290\n",
      "49\n",
      "Done with article 10 of 290\n",
      "141\n",
      "Done with article 11 of 290\n",
      "32\n",
      "Done with article 12 of 290\n",
      "25\n",
      "Done with article 13 of 290\n",
      "6\n",
      "Done with article 14 of 290\n",
      "18\n",
      "Done with article 15 of 290\n",
      "20\n",
      "Done with article 16 of 290\n",
      "16\n",
      "Done with article 17 of 290\n",
      "25\n",
      "Done with article 18 of 290\n",
      "70\n",
      "Done with article 19 of 290\n",
      "92\n",
      "Done with article 20 of 290\n",
      "68\n",
      "Done with article 21 of 290\n",
      "As an immunologist that is interested in quantum computing I’m quite sure that I’m “not really good at anything”. I’m sure I can’t understand Shor’s algorythm – neither in 30 min nor 30 months, but what I do understand is that insulting colleagues on the internet is unprofessional and that your worldview has long since ceased to have a future. Still, I’d be very interested in reading your current paper – if you had any. https://scottaaronson.blog/?p=6457\n",
      "57\n",
      "Done with article 22 of 290\n",
      "43\n",
      "Done with article 23 of 290\n",
      "17\n",
      "Done with article 24 of 290\n",
      "313\n",
      "Done with article 25 of 290\n",
      "105\n",
      "Done with article 26 of 290\n",
      "11\n",
      "Done with article 27 of 290\n",
      "32\n",
      "Done with article 28 of 290\n",
      "64\n",
      "Done with article 29 of 290\n",
      "NotJudging:\n",
      "“A broken clock is right twice a day and a constant risk adverse attitude is going to be correct every so often.”\n",
      "I see myself described here :).\n",
      "However, what do you think of my comment #10? I’d love to know. https://scottaaronson.blog/?p=6098\n",
      "9\n",
      "Done with article 30 of 290\n",
      "15\n",
      "Done with article 31 of 290\n",
      "40\n",
      "Done with article 32 of 290\n",
      "29\n",
      "Done with article 33 of 290\n",
      "43\n",
      "Done with article 34 of 290\n",
      "16\n",
      "Done with article 35 of 290\n",
      "24\n",
      "Done with article 36 of 290\n",
      "18\n",
      "Done with article 37 of 290\n",
      "26\n",
      "Done with article 38 of 290\n",
      "22\n",
      "Done with article 39 of 290\n",
      "25\n",
      "Done with article 40 of 290\n",
      "17\n",
      "Done with article 41 of 290\n",
      "18\n",
      "Done with article 42 of 290\n",
      "23\n",
      "Done with article 43 of 290\n",
      "70\n",
      "Done with article 44 of 290\n",
      "113\n",
      "Done with article 45 of 290\n",
      "from Jelmer Renema is spot on. The question is should bad actors waving their arms and shouting “Quantum Supremacy” be taking money from dentists in La Jolla (as we say in the biz). YMMV. https://scottaaronson.blog/?p=5387\n",
      "96\n",
      "Done with article 46 of 290\n",
      "5\n",
      "Done with article 47 of 290\n",
      "18\n",
      "Done with article 48 of 290\n",
      "23\n",
      "Done with article 49 of 290\n",
      "138\n",
      "Done with article 50 of 290\n",
      "13\n",
      "Done with article 51 of 290\n",
      "24\n",
      "Done with article 52 of 290\n",
      "27\n",
      "Done with article 53 of 290\n",
      "13\n",
      "Done with article 54 of 290\n",
      "66\n",
      "Done with article 55 of 290\n",
      "16\n",
      "Done with article 56 of 290\n",
      "3\n",
      "Done with article 57 of 290\n",
      "20\n",
      "Done with article 58 of 290\n",
      "36\n",
      "Done with article 59 of 290\n",
      "25\n",
      "Done with article 60 of 290\n",
      "3\n",
      "Done with article 61 of 290\n",
      "25\n",
      "Done with article 62 of 290\n",
      "26\n",
      "Done with article 63 of 290\n",
      "30\n",
      "Done with article 64 of 290\n",
      "36\n",
      "Done with article 65 of 290\n",
      "25\n",
      "Done with article 66 of 290\n",
      "47\n",
      "Done with article 67 of 290\n",
      "40\n",
      "Done with article 68 of 290\n",
      "46\n",
      "Done with article 69 of 290\n",
      "21\n",
      "Done with article 70 of 290\n",
      "7\n",
      "Done with article 71 of 290\n",
      "24\n",
      "Done with article 72 of 290\n",
      "30\n",
      "Done with article 73 of 290\n",
      "26\n",
      "Done with article 74 of 290\n",
      "84\n",
      "Done with article 75 of 290\n",
      "35\n",
      "Done with article 76 of 290\n",
      "126\n",
      "Done with article 77 of 290\n",
      "13\n",
      "Done with article 78 of 290\n",
      "9\n",
      "Done with article 79 of 290\n",
      "9\n",
      "Done with article 80 of 290\n",
      "42\n",
      "Done with article 81 of 290\n",
      "58\n",
      "Done with article 82 of 290\n",
      "132\n",
      "Done with article 83 of 290\n",
      "152\n",
      "Done with article 84 of 290\n",
      "40\n",
      "Done with article 85 of 290\n",
      "41\n",
      "Done with article 86 of 290\n",
      "13\n",
      "Done with article 87 of 290\n",
      "12\n",
      "Done with article 88 of 290\n",
      "18\n",
      "Done with article 89 of 290\n",
      "43\n",
      "Done with article 90 of 290\n",
      "12\n",
      "Done with article 91 of 290\n",
      "25\n",
      "Done with article 92 of 290\n",
      "8\n",
      "Done with article 93 of 290\n",
      "42\n",
      "Done with article 94 of 290\n",
      "16\n",
      "Done with article 95 of 290\n",
      "18\n",
      "Done with article 96 of 290\n",
      "32\n",
      "Done with article 97 of 290\n",
      "66\n",
      "Done with article 98 of 290\n",
      "32\n",
      "Done with article 99 of 290\n",
      "52\n",
      "Done with article 100 of 290\n",
      "106\n",
      "Done with article 101 of 290\n",
      "5\n",
      "Done with article 102 of 290\n",
      "13\n",
      "Done with article 103 of 290\n",
      "62\n",
      "Done with article 104 of 290\n",
      "19\n",
      "Done with article 105 of 290\n",
      "41\n",
      "Done with article 106 of 290\n",
      "17\n",
      "Done with article 107 of 290\n",
      "21\n",
      "Done with article 108 of 290\n",
      "26\n",
      "Done with article 109 of 290\n",
      "40\n",
      "Done with article 110 of 290\n",
      "146\n",
      "Done with article 111 of 290\n",
      "– i don’t understand. the alien offers to answer ONE question, the professors ask the alien TWO questions (expressed in a single sentence), and the alien answers BOTH questions. are the alien and the professors all innumerate? https://scottaaronson.blog/?p=3638\n",
      "44\n",
      "Done with article 112 of 290\n",
      "100\n",
      "Done with article 113 of 290\n",
      "41\n",
      "Done with article 114 of 290\n",
      "47\n",
      "Done with article 115 of 290\n",
      "29\n",
      "Done with article 116 of 290\n",
      "17\n",
      "Done with article 117 of 290\n",
      "85\n",
      "Done with article 118 of 290\n",
      "– Turns out that the initial placement of all those rocks was … https://scottaaronson.blog/?p=3482\n",
      "35\n",
      "Done with article 119 of 290\n",
      "35\n",
      "Done with article 120 of 290\n",
      "101\n",
      "Done with article 121 of 290\n",
      "29\n",
      "Done with article 122 of 290\n",
      "6\n",
      "Done with article 123 of 290\n",
      "36\n",
      "Done with article 124 of 290\n",
      "15\n",
      "Done with article 125 of 290\n",
      "32\n",
      "Done with article 126 of 290\n",
      "53\n",
      "Done with article 127 of 290\n",
      "25\n",
      "Done with article 128 of 290\n",
      "22\n",
      "Done with article 129 of 290\n",
      "37\n",
      "Done with article 130 of 290\n",
      "40\n",
      "Done with article 131 of 290\n",
      "30\n",
      "Done with article 132 of 290\n",
      "11\n",
      "Done with article 133 of 290\n",
      "8\n",
      "Done with article 134 of 290\n",
      "14\n",
      "Done with article 135 of 290\n",
      "176\n",
      "Done with article 136 of 290\n",
      "63\n",
      "Done with article 137 of 290\n",
      "52\n",
      "Done with article 138 of 290\n",
      "76\n",
      "Done with article 139 of 290\n",
      "NP or PH or even NEXP in non-uniform TC0 is consistent with all the uniform conjectures in complexity theory (it does not move the needle even a bit) like P=NP millenium problem.\n",
      "Doesn’t this seem odd and not indicate that PH in non-uniform TC0 is fair odds (as a bonus it may even indicate PSPACE and PH are distinct)? https://scottaaronson.blog/?p=2612\n",
      "58\n",
      "Done with article 140 of 290\n",
      "21\n",
      "Done with article 141 of 290\n",
      "122\n",
      "Done with article 142 of 290\n",
      "24\n",
      "Done with article 143 of 290\n",
      "41\n",
      "Done with article 144 of 290\n",
      "77\n",
      "Done with article 145 of 290\n",
      "52\n",
      "Done with article 146 of 290\n",
      "34\n",
      "Done with article 147 of 290\n",
      "12\n",
      "Done with article 148 of 290\n",
      "51\n",
      "Done with article 149 of 290\n",
      "25\n",
      "Done with article 150 of 290\n",
      "35\n",
      "Done with article 151 of 290\n",
      "37\n",
      "Done with article 152 of 290\n",
      "14\n",
      "Done with article 153 of 290\n",
      "43\n",
      "Done with article 154 of 290\n",
      "amounts to (see also Bremner-Jozsa-Shepherd, arXiv:1005.1407 [quant-ph], also Aaronson-Arkhipov, arXiv:1011.3245 [quant-ph])\n",
      "which naturally generalizes to\n",
      "Questions Aren’t the CT and ECT both flawed (as stated)?\\(\\ \\)… and for similar reasons?\n",
      "Falsifiable assertion What complexity theorists reasonably regard as a “really, really good” sampling simulation differs fundamentally from what experimentalists and statisticians reasonably regard as a “really, really good” sampling simulation\\(\\ \\)… and this definitional imprecision has fueled public and scientific discourse of deplorable confusion and futility.\n",
      "`t\\(\\ \\)Hooft’s advice In view of these definitional ambiguities, `t\\(\\ \\)Hooft’s advice surely applies (per arXiv:1405.1548)\n",
      "The history of scientific research in general, and of quantum complexity research in particular, shows us plainly that `t\\(\\ \\)Hooft’s advice is worthy of consideration by complexity theorists, experimentalists, and statisticians alike.\n",
      "Conclusion The formal complexity-theory barriers to the computational simulation of random-number distributions and boson-sampling distributions should not dissuade us from implementing either one, with reasonable hope that for practical purposes our simulations can be both efficient and “really, really good”. https://scottaaronson.blog/?p=2070\n",
      "28\n",
      "Done with article 155 of 290\n",
      "35\n",
      "Done with article 156 of 290\n",
      "92\n",
      "Done with article 157 of 290\n",
      "35\n",
      "Done with article 158 of 290\n",
      "95\n",
      "Done with article 159 of 290\n",
      "31\n",
      "Done with article 160 of 290\n",
      "33\n",
      "Done with article 161 of 290\n",
      "29\n",
      "Done with article 162 of 290\n",
      "68\n",
      "Done with article 163 of 290\n",
      "113\n",
      "Done with article 164 of 290\n",
      "77\n",
      "Done with article 165 of 290\n",
      "284\n",
      "Done with article 166 of 290\n",
      "31\n",
      "Done with article 167 of 290\n",
      "44\n",
      "Done with article 168 of 290\n",
      "98\n",
      "Done with article 169 of 290\n",
      "46\n",
      "Done with article 170 of 290\n",
      "28\n",
      "Done with article 171 of 290\n",
      "55\n",
      "Done with article 172 of 290\n",
      "“Some discoveries are important, change the way how “almost” everything is thought about or how everything is calculated; others don’t.”\n",
      "I will bet this amplituhedron isn’t one of those.\n",
      "I will further wager this as flavor of the month (or year). Wait for a year or two and this will sound less revolutionary than people make it out to be right now. https://scottaaronson.blog/?p=1537\n",
      "117\n",
      "Done with article 173 of 290\n",
      "64\n",
      "Done with article 174 of 290\n",
      "19\n",
      "Done with article 175 of 290\n",
      "382\n",
      "Done with article 176 of 290\n",
      "17\n",
      "Done with article 177 of 290\n",
      "20\n",
      "Done with article 178 of 290\n",
      "13\n",
      "Done with article 179 of 290\n",
      "41\n",
      "Done with article 180 of 290\n",
      "52\n",
      "Done with article 181 of 290\n",
      "17\n",
      "Done with article 182 of 290\n",
      "90\n",
      "Done with article 183 of 290\n",
      "19\n",
      "Done with article 184 of 290\n",
      "77\n",
      "Done with article 185 of 290\n",
      "118\n",
      "Done with article 186 of 290\n",
      "19\n",
      "Done with article 187 of 290\n",
      "23\n",
      "Done with article 188 of 290\n",
      "57\n",
      "Done with article 189 of 290\n",
      "105\n",
      "Done with article 190 of 290\n",
      "13\n",
      "Done with article 191 of 290\n",
      "92\n",
      "Done with article 192 of 290\n",
      "340\n",
      "Done with article 193 of 290\n",
      "57\n",
      "Done with article 194 of 290\n",
      "140\n",
      "Done with article 195 of 290\n",
      "20\n",
      "Done with article 196 of 290\n",
      "40\n",
      "Done with article 197 of 290\n",
      "15\n",
      "Done with article 198 of 290\n",
      "48\n",
      "Done with article 199 of 290\n",
      "124\n",
      "Done with article 200 of 290\n",
      "26\n",
      "Done with article 201 of 290\n",
      "36\n",
      "Done with article 202 of 290\n",
      "19\n",
      "Done with article 203 of 290\n",
      "79\n",
      "Done with article 204 of 290\n",
      "95\n",
      "Done with article 205 of 290\n",
      "24\n",
      "Done with article 206 of 290\n",
      "18\n",
      "Done with article 207 of 290\n",
      "30\n",
      "Done with article 208 of 290\n",
      "20\n",
      "Done with article 209 of 290\n",
      "78\n",
      "Done with article 210 of 290\n",
      "22\n",
      "Done with article 211 of 290\n",
      "41\n",
      "Done with article 212 of 290\n",
      "“They said you were a shameless self-promoter, which this post seems to confirm!”\n",
      "By that definition, jliptron is a “shameless self-promoter” as well; promoting false proofs to get visitors to his blog… well at least aaron scottson shamelessly self-promoted himself but he was proved right that the proof shamelessly promoted by the other shameless self-promoter was wrong!\n",
      "Then there are the “shameless self-promoters” who claim “confirmations have already started arriving.” as soon as the email server sent a TCP/IP ACK signal! Strangely today that shameless self-promoter’s proofs have been pulled from his website? But some of his uncritical supporters have not given up yet. https://scottaaronson.blog/?p=461\n",
      "66\n",
      "Done with article 213 of 290\n",
      "58\n",
      "Done with article 214 of 290\n",
      "36\n",
      "Done with article 215 of 290\n",
      "56\n",
      "Done with article 216 of 290\n",
      "47\n",
      "Done with article 217 of 290\n",
      "39\n",
      "Done with article 218 of 290\n",
      "30\n",
      "Done with article 219 of 290\n",
      "52\n",
      "Done with article 220 of 290\n",
      "87\n",
      "Done with article 221 of 290\n",
      "56\n",
      "Done with article 222 of 290\n",
      "17\n",
      "Done with article 223 of 290\n",
      "57\n",
      "Done with article 224 of 290\n",
      "29\n",
      "Done with article 225 of 290\n",
      "22\n",
      "Done with article 226 of 290\n",
      "20\n",
      "Done with article 227 of 290\n",
      "65\n",
      "Done with article 228 of 290\n",
      "63\n",
      "Done with article 229 of 290\n",
      "59\n",
      "Done with article 230 of 290\n",
      "25\n",
      "Done with article 231 of 290\n",
      "29\n",
      "Done with article 232 of 290\n",
      "64\n",
      "Done with article 233 of 290\n",
      "96\n",
      "Done with article 234 of 290\n",
      "43\n",
      "Done with article 235 of 290\n",
      "120\n",
      "Done with article 236 of 290\n",
      "28\n",
      "Done with article 237 of 290\n",
      "103\n",
      "Done with article 238 of 290\n",
      "124\n",
      "Done with article 239 of 290\n",
      "18\n",
      "Done with article 240 of 290\n",
      "58\n",
      "Done with article 241 of 290\n",
      "157\n",
      "Done with article 242 of 290\n",
      "72\n",
      "Done with article 243 of 290\n",
      "16\n",
      "Done with article 244 of 290\n",
      "282\n",
      "Done with article 245 of 290\n",
      "16\n",
      "Done with article 246 of 290\n",
      "27\n",
      "Done with article 247 of 290\n",
      "59\n",
      "Done with article 248 of 290\n",
      "6\n",
      "Done with article 249 of 290\n",
      "33\n",
      "Done with article 250 of 290\n",
      "10\n",
      "Done with article 251 of 290\n",
      "9\n",
      "Done with article 252 of 290\n",
      "39\n",
      "Done with article 253 of 290\n",
      "20\n",
      "Done with article 254 of 290\n",
      "4\n",
      "Done with article 255 of 290\n",
      "43\n",
      "Done with article 256 of 290\n",
      "26\n",
      "Done with article 257 of 290\n",
      "106\n",
      "Done with article 258 of 290\n",
      "39\n",
      "Done with article 259 of 290\n",
      "34\n",
      "Done with article 260 of 290\n",
      "62\n",
      "Done with article 261 of 290\n",
      "25\n",
      "Done with article 262 of 290\n",
      "77\n",
      "Done with article 263 of 290\n",
      "42\n",
      "Done with article 264 of 290\n",
      "55\n",
      "Done with article 265 of 290\n",
      "64\n",
      "Done with article 266 of 290\n",
      "13\n",
      "Done with article 267 of 290\n",
      "9\n",
      "Done with article 268 of 290\n",
      "10\n",
      "Done with article 269 of 290\n",
      "20\n",
      "Done with article 270 of 290\n",
      "36\n",
      "Done with article 271 of 290\n",
      "13\n",
      "Done with article 272 of 290\n",
      "127\n",
      "Done with article 273 of 290\n",
      "30\n",
      "Done with article 274 of 290\n",
      "16\n",
      "Done with article 275 of 290\n",
      "4\n",
      "Done with article 276 of 290\n",
      "60\n",
      "Done with article 277 of 290\n",
      "8\n",
      "Done with article 278 of 290\n",
      "6\n",
      "Done with article 279 of 290\n",
      "36\n",
      "Done with article 280 of 290\n",
      "40\n",
      "Done with article 281 of 290\n",
      "19\n",
      "Done with article 282 of 290\n",
      "22\n",
      "Done with article 283 of 290\n",
      "49\n",
      "Done with article 284 of 290\n",
      "57\n",
      "Done with article 285 of 290\n",
      "27\n",
      "Done with article 286 of 290\n",
      "43\n",
      "Done with article 287 of 290\n",
      "26\n",
      "Done with article 288 of 290\n",
      "54\n",
      "Done with article 289 of 290\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "def cmt_date_body_parse(cmt, url):\n",
    "    try:\n",
    "        return {\"date\": datetime.strptime(re.sub(r\"((st)|(nd)|(rd)|(th)),\", \"\", cmt.split(\"\\n\")[0]), \"%B %d %Y at %I:%M %p\").date(), \"body\": \" \\n\".join(cmt.split(\"\\n\")[1:])}\n",
    "    except:\n",
    "        print(cmt,url)\n",
    "        return None\n",
    "\n",
    "num_urls = len(url_list)\n",
    "article_instances = []\n",
    "article_ids = []\n",
    "\n",
    "for (iu, url) in enumerate(url_list):\n",
    "    if validate_url(url) != None:\n",
    "        if validate_url(url)[0]: #check whether url can be resolved or is damaged\n",
    "            downloaded = fetch_url(url)\n",
    "            result = bare_extraction(downloaded, \n",
    "                                        url=url, \n",
    "                                        include_comments=True,\n",
    "                                        #include_formatting=False,\n",
    "                                        favor_precision=True, #favor recall\n",
    "                                        with_metadata=True,\n",
    "                                        #deduplicate=True, \n",
    "                                        only_with_metadata=True,\n",
    "                                        #url_blacklist=None,\n",
    "                                        as_dict=True,\n",
    "                                        )\n",
    "            art_id = url.split(\"?p=\")[-1]\n",
    "            \n",
    "            comments = re.split(r\"\\nComment\\s#[0-9]+\\s\", string=result[\"comments\"])[1:]\n",
    "            comment_dicts = [cmt_date_body_parse(cmt, url) for cmt in comments] \n",
    "            comment_dicts = [cmt_dict for cmt_dict in comment_dicts if cmt_dict != None]\n",
    "            tl_comment_dicts = [cmt for cmt in comment_dicts if \"#\" not in \"\".join(cmt[\"body\"].split(\"\\n\")[0:1]) ]\n",
    "            print(len(tl_comment_dicts))\n",
    "            comment_instances = []\n",
    "            comment_ids = []\n",
    "            for cmt_dict in tl_comment_dicts:\n",
    "                custom_cmt_id = hashlib.sha256(cmt_dict[\"body\"].encode(\"utf-8\")).hexdigest()\n",
    "                cmt_inst = dm.Comment(comment_id=custom_cmt_id,\n",
    "                            article_id=art_id,\n",
    "                            date=cmt_dict[\"date\"],\n",
    "                            username=\"Unknown\",\n",
    "                            from_where=\"Shtetl-Optimized\",\n",
    "                            body=cmt_dict[\"body\"],\n",
    "                            level=0,\n",
    "                            parent_comment_id=None,\n",
    "                            replies=[],\n",
    "                            replies_ids=[])\n",
    "                comment_instances.append(cmt_inst)\n",
    "                comment_ids.append(custom_cmt_id)\n",
    "            \n",
    "            art_inst = dm.Article(article_id=art_id,\n",
    "                        article_type=\"blog_article\",\n",
    "                        link=url,\n",
    "                        post_id=[\"N.A.\", ],\n",
    "                        posted_by=[\"N.A.\", ],\n",
    "                        date_posted=[\"N.A.\", ],\n",
    "                        post_title=[\"N.A.\", ],\n",
    "                        post_body=[\"N.A.\", ],\n",
    "                        date= result[\"date\"],\n",
    "                        author=\"Scott Aaronson\",\n",
    "                        headline=result[\"title\"],\n",
    "                        body=result[\"text\"],\n",
    "                        comments=comment_instances,\n",
    "                        comments_ids=comment_ids)\n",
    "            article_instances.append(art_inst)\n",
    "            article_ids.append(art_id)\n",
    "    print(\"Done with article %d of %d\" % (iu, num_urls))\n",
    "\n",
    "artset = dm.ArticleSet(label=\"Shtetl-Optimized: Quantum Category\",\n",
    "              as_list=article_instances,\n",
    "              article_id_list=article_ids,)\n",
    "\n",
    "filepath = \"S:\\\\Sync\\\\University\\\\2023_MRP_1\\\\MRP1_WorkDir\\\\data\\\\\"\n",
    "artset.save_to_csv(filepath+\"shtetl_optim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'url' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6832/861176323.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnewspaper\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mArticle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArticle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'url' is not defined"
     ]
    }
   ],
   "source": [
    "#test newspaer package for url parsing:\n",
    "from newspaper import Article\n",
    "\n",
    "a = Article(url)\n",
    "a.download()\n",
    "a.parse()\n",
    "#print(a.summary)\n",
    "print(a.title, a.authors, a.publish_date)\n",
    "print(\"Text: \", a.text)\n",
    "\n",
    "#print(a.nlp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test readabilipy package for url parsing:\n",
    "import requests\n",
    "from readabilipy import simple_json_from_html_string\n",
    "req = requests.get(url)\n",
    "article = simple_json_from_html_string(req.text, use_readability=True)\n",
    "#print(article.keys())\n",
    "print(article[\"title\"], article[\"byline\"], article[\"date\"] )\n",
    "print(article[\"plain_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import youtube_api as yapi\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import data_struct as dm\n",
    "\n",
    "\n",
    "#test = yapi.video_search(\"quantum\")\n",
    "\n",
    "video_dict = {'youID':[], 'title':[], 'pub_date':[],'channel_name':[], \"video_description\":[]}\n",
    "\"\"\"\n",
    "just_json = test[1]\n",
    "\n",
    "for video in just_json:\n",
    "    print(video['id']['videoId'])\n",
    "    print(video['snippet']['title'])\n",
    "    print(video['snippet']['publishedAt'])\n",
    "    print(video['snippet']['channelTitle'])\n",
    "    print(video['snippet']['description'])\n",
    "\"\"\"\n",
    "\n",
    "def grab_videos(keyword, token=None):\n",
    "    res = yapi.video_search(q=keyword, \n",
    "                         order=\"relevance\",\n",
    "                         token=token,\n",
    "                         )\n",
    "    c_token = res[0]\n",
    "    c_videos = res[1]\n",
    "    for vid in c_videos:\n",
    "        video_dict['youID'].append(vid['id']['videoId'])\n",
    "        video_dict['title'].append(vid['snippet']['title'])\n",
    "        video_dict['pub_date'].append(vid['snippet']['publishedAt'])\n",
    "    print(\"added \", str(len(c_videos)),  \" videos to a total of \", str(len(video_dict['youID'])))\n",
    "    return token\n",
    "\n",
    "\n",
    "token = grab_videos(\"quantum\")\n",
    "while token != \"last_page\":\n",
    "    token = grab_videos(\"quantum\", token=token)\n",
    "\n",
    "\n",
    "res = yapi.video_comments_query(\"Mz5gwDykgfA\", token=None)\n",
    "print(len(res[1]), len(res[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5af6d6db34e8ca116fae08f6a59dbf71471a55a5411e623a27fad945680c9636"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
