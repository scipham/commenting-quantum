{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\trothe\\.julia\\conda\\3\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (None)/charset_normalizer (3.0.1) doesn't match a supported version!\r\n",
      "  warnings.warn(\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "from_reddit_dfs (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "include(\"data_struct.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in subreddit: science..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in subreddit: quantum..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donxe with processing post 1Donxe with processing post 2"
     ]
    }
   ],
   "source": [
    "scraper = Scraper(reddit_api_wrapper=\"praw\")\n",
    "post_df, comment_df = query_from_subreddits(scraper,    \n",
    "                                            query=\"quantum self:no\",\n",
    "                                            subreddits=[\"science\", \"quantum\"],     \n",
    "                                            post_limit_per_subreddit=20, \n",
    "                                            sort_by=\"relevance\",\n",
    "                                            )\n",
    "\n",
    "\n",
    "filepath = \"S:\\\\Sync\\\\University\\\\2023_MRP_1\\\\MRP1_WorkDir\\\\data\\\\\"\n",
    "#Convert to a standardized object structure before exporting to csv files:\n",
    "artset = from_reddit_dfs(artset_descript=\"Test ArticleSet\", \n",
    "                        post_df = post_df, \n",
    "                        comment_df = comment_df,\n",
    "                        scraper=scraper,)\n",
    "\n",
    "#print(artset.as_list[0].comments[2])\n",
    "\n",
    "#artset.save_to_csv(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df[\"depth\"]\n",
    "\n",
    "from math import modf\n",
    "math.modf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR store directly in csv files via a direct mapping:\n",
    "reddit_postFeatures = [\"id\",\"ext_link_url\"]\n",
    "reddit_commentFeatures = [\"id\", \"parent_post_id\", \"created_utc\", \"body\", \"depth\",\"parent_comment_id\" , \"replies_ids\"]\n",
    "\n",
    "target_articleFeatures = [\"article_id\", \"article_type\", \"link\", \"date\", \"author\", \"headline\", \"body\", \"char_count\", \"comments_ids\"] # [field.name for field in fields(Article)].remove(\"comments\")\n",
    "target_commentFeatures =  [\"comment_id\", \"article_id\", \"date\", \"username\", \"from_where\" ,\"body\", \"level\" , \"parent_comment_id\", \"replies_ids\"]\n",
    "direct_target_articleFeatures = target_articleFeatures.copy().remove(\"article_type\", \"date\", \"author\", \"headline\", \"body\", \"char_count\", \"comments_ids\") #Subset of target_articleFeatures which can be directly mapped from the reddit data\n",
    "direct_target_commentFeatures = target_commentFeatures.copy() #Subset of target_commentFeatures which can be directly mapped from the reddit data\n",
    "\n",
    "reddit_to_commentdf_map = dict(zip(direct_target_articleFeatures, reddit_postFeatures))\n",
    "reddit_to_articledf_map = dict(zip(direct_target_commentFeatures, reddit_commentFeatures))\n",
    "\n",
    "export_article_df = pd.DataFrame(columns=target_articleFeatures)\n",
    "export_comment_df = pd.DataFrame(columns=target_commentFeatures)\n",
    "export_article_df.loc[:, reddit_to_articledf_map.keys()] = post_df.loc[:, reddit_to_articledf_map.values()]\n",
    "export_comment_df.loc[:, reddit_to_commentdf_map.keys()] = comment_df.loc[:, reddit_to_commentdf_map.values()]\n",
    "\n",
    "export_article_df.loc[:,\"article_type\"] = \"web_article\"\n",
    "#Parse: \"date\", \"author\", \"headline\", \"body\" from ext link url\n",
    "# \n",
    "export_article_df.loc[:,\"char_count\"] = len(export_article_df.loc[:,\"body\"]) \n",
    "# Map \"comments_ids\" to the articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trafilatura import fetch_url, extract, bare_extraction\n",
    "from courlan import validate_url, check_url, clean_url\n",
    "import requests\n",
    "\n",
    "#url = 'https://github.blog/2019-03-29-leader-spotlight-erin-spiceland/'\n",
    "url = \"https://scottaaronson.blog/?p=6871\"\n",
    "url = \"https://tweakers.net/nieuws/189740/qutech-start-quantum-network-explorer-voor-publieke-experimenten-quantumnetwerk.html\"\n",
    "url = \"https://www.quantamagazine.org/how-quantum-physicists-flipped-time-and-how-they-didnt-20230127/\"\n",
    "#url = \"https://news.st-andrews.ac.uk/archive/st-andrews-scientists-turn-up-the-heat-on-physics-phenomenon/#:~:text=A%20'quantum%20harmonic%20oscillator'%20%E2%80%93,the%20University%20of%20St%20Andrews.\"\n",
    "#url = \"https://www.vice.com/en/article/88qj3z/government-scientists-discover-entirely-new-kind-of-quantum-entanglement-in-breakthrough\"\n",
    "#url = \"https://www.nature.com/articles/s41534-022-00631-2\"\n",
    "url = \"https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiX073B8vH8AhUThP0HHe5FDd8QtwJ6BAgSEAI&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DjHoEjvuPoB8&usg=AOvVaw2oWlY57R1_JHg0bgTgPkoW\"\n",
    "url = \"https://youtu.be/jHoEjvuPoB8\"\n",
    "\n",
    "assert validate_url(url)[0] == True #check whether url can be resolved or is damaged\n",
    "\n",
    "url = clean_url(url)\n",
    "downloaded = fetch_url(url)\n",
    "\n",
    "result = bare_extraction(downloaded, \n",
    "                            url=url, \n",
    "                            #include_comments=True,\n",
    "                            #include_formatting=False,\n",
    "                            favor_precision=True, #favor recall\n",
    "                            with_metadata=True,\n",
    "                            #deduplicate=True, \n",
    "                            only_with_metadata=True,\n",
    "                            #url_blacklist=None,\n",
    "                            as_dict=True,\n",
    "                            )\n",
    "\n",
    "#print(result.keys())\n",
    "print(result[\"title\"], \"...\", result[\"author\"],\"...\",  result[\"date\"])\n",
    "#print(result[\"description\"])\n",
    "print(result[\"text\"])\n",
    "\n",
    "\n",
    "#extract_comments()\n",
    "#sanitize()\n",
    "#trim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "\n",
    "a = Article(url)\n",
    "a.download()\n",
    "a.parse()\n",
    "#print(a.summary)\n",
    "print(a.title, a.authors, a.publish_date)\n",
    "print(\"Text: \", a.text)\n",
    "\n",
    "#print(a.nlp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from readabilipy import simple_json_from_html_string\n",
    "req = requests.get(url)\n",
    "article = simple_json_from_html_string(req.text, use_readability=True)\n",
    "#print(article.keys())\n",
    "print(article[\"title\"], article[\"byline\"], article[\"date\"] )\n",
    "print(article[\"plain_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.5",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5af6d6db34e8ca116fae08f6a59dbf71471a55a5411e623a27fad945680c9636"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
